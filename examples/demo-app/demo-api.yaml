# Demo API Application
# A simple FastAPI service that simulates a real-world API
#
# This deployment is intentionally set up to be "breakable" for demo scenarios.
# The healthy version includes all required environment variables.

apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-api-config
  namespace: production
  labels:
    app: demo-api
    app.kubernetes.io/part-of: aegis-demo
data:
  LOG_LEVEL: "INFO"
  WORKERS: "2"
  APP_NAME: "AEGIS Demo API"
---
apiVersion: v1
kind: Secret
metadata:
  name: demo-api-secrets
  namespace: production
  labels:
    app: demo-api
type: Opaque
stringData:
  # This is a demo secret - in real scenarios, use external secrets
  DATABASE_URL: "postgresql://demo:demo123@demo-db:5432/demoapp" # pragma: allowlist secret - Demo credentials for local development only
  REDIS_URL: "redis://demo-redis:6379/0"
  API_KEY: "demo-api-key-not-for-production" # pragma: allowlist secret - Demo API key for local testing only
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-api
  namespace: production
  labels:
    app: demo-api
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/component: api
    app.kubernetes.io/part-of: aegis-demo
  annotations:
    aegis.io/auto-remediate: "true"
    aegis.io/severity-threshold: "medium"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: api
          # Using a lightweight Python image
          image: python:3.12-slim
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              echo "Starting up (Standard Lib Version)..."
              mkdir -p /app
              cat > /app/main.py << 'EOF'
              import http.server
              import socketserver
              import os
              import time
              import threading
              import json
              import random

              APP_NAME = os.getenv("APP_NAME", "Demo API")
              PORT = 8000

              # Simple Metrics Store
              metrics = {
                  "http_requests_total": {},
                  "http_request_duration_seconds_sum": {},
                  "http_request_duration_seconds_count": {}
              }

              def inc_counter(name, labels):
                  key = tuple(sorted(labels.items()))
                  if key not in metrics[name]:
                      metrics[name][key] = 0.0
                  metrics[name][key] += 1.0

              def observe_histogram(name, labels, value):
                  # Simple sum/count for histogram
                  key = tuple(sorted(labels.items()))

                  sum_name = f"{name}_sum"
                  count_name = f"{name}_count"

                  if key not in metrics[sum_name]:
                      metrics[sum_name][key] = 0.0
                  if key not in metrics[count_name]:
                      metrics[count_name][key] = 0.0

                  metrics[sum_name][key] += value
                  metrics[count_name][key] += 1.0

              class DemoHandler(http.server.BaseHTTPRequestHandler):
                  def do_GET(self):
                      start_time = time.time()
                      status = 200

                      try:
                          if self.path == "/":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"status": "healthy", "app": APP_NAME}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/health":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"status": "ok", "timestamp": time.time()}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/ready":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"ready": True, "database": "connected", "redis": "connected"}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/api/items":
                              # Simulate DB latency
                              time.sleep(random.uniform(0.01, 0.05))
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"items": [{"id": 1, "name": "Demo Item"}]}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/metrics":
                              self.send_response(200)
                              self.send_header("Content-type", "text/plain")
                              self.end_headers()

                              # Generate Prometheus format
                              output = []

                              # Counter
                              output.append("# HELP http_requests_total Total HTTP requests")
                              output.append("# TYPE http_requests_total counter")
                              for key, value in metrics["http_requests_total"].items():
                                  label_str = ",".join([f'{k}="{v}"' for k, v in key])
                                  output.append(f'http_requests_total{{{label_str}}} {value}')

                              # Histogram (simplified)
                              output.append("# HELP http_request_duration_seconds HTTP request latency")
                              output.append("# TYPE http_request_duration_seconds histogram")
                              for name in ["http_request_duration_seconds_sum", "http_request_duration_seconds_count"]:
                                  for key, value in metrics[name].items():
                                      label_str = ",".join([f'{k}="{v}"' for k, v in key])
                                      output.append(f'{name}{{{label_str}}} {value}')

                              self.wfile.write("\n".join(output).encode())
                              return # Don't record metrics for metrics endpoint

                          else:
                              self.send_response(404)
                              self.end_headers()
                              status = 404

                          # Record metrics
                          duration = time.time() - start_time
                          labels = {"method": "GET", "endpoint": self.path, "status": str(status)}
                          inc_counter("http_requests_total", labels)

                          # Histogram labels usually less cardinality
                          hist_labels = {"endpoint": self.path}
                          observe_histogram("http_request_duration_seconds", hist_labels, duration)

                      except Exception as e:
                          print(f"Error: {e}")
                          if not self.wfile.closed:
                              self.send_response(500)
                              self.end_headers()

              class ThreadedHTTPServer(socketserver.ThreadingMixIn, http.server.HTTPServer):
                  daemon_threads = True

              if __name__ == "__main__":
                  print(f"Starting {APP_NAME} on port {PORT}")
                  server = ThreadedHTTPServer(('0.0.0.0', PORT), DemoHandler)
                  server.serve_forever()
              EOF
              python /app/main.py
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          envFrom:
            - configMapRef:
                name: demo-api-config
            - secretRef:
                name: demo-api-secrets
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: demo-api
  namespace: production
  labels:
    app: demo-api
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8000
      protocol: TCP
      name: http
  selector:
    app: demo-api
---
apiVersion: v1
kind: Service
metadata:
  name: demo-api-nodeport
  namespace: production
  labels:
    app: demo-api
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8000
      nodePort: 30000
      protocol: TCP
      name: http
  selector:
    app: demo-api
