# INCIDENT: OOMKilled - Memory Leak Simulation
#
# Scenario: Deploy demo-api with a memory leak that exceeds limits
# Expected: Pod gets OOMKilled after ~30 seconds
#
# AEGIS Expected Detection:
#   Root Cause: Container exceeded memory limits (128Mi)
#   Severity: HIGH
#   Fix: Increase memory limits or fix memory leak in application
#
# Usage:
#   kubectl apply -f oomkill-memory-leak.yaml
#   kubectl get pods -n production -w  # Watch OOMKilled status

apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-api
  namespace: production
  labels:
    app: demo-api
    incident-type: oomkill
  annotations:
    aegis.io/incident-scenario: "oomkill-memory-leak"
    aegis.io/expected-fix: "Increase memory limits to 512Mi or fix memory leak"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
        version: v1-oom
    spec:
      containers:
        - name: api
          image: python:3.12-slim
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              echo "Starting up (Standard Lib Version + Leak)..."
              mkdir -p /app
              cat > /app/main.py << 'EOF'
              import http.server
              import socketserver
              import os
              import time
              import threading
              import json
              import random

              APP_NAME = os.getenv("APP_NAME", "Demo API")
              PORT = 8000

              # Simple Metrics Store
              metrics = {
                  "http_requests_total": {},
                  "http_request_duration_seconds_sum": {},
                  "http_request_duration_seconds_count": {}
              }

              # Memory Leak Simulation
              LEAKY_LIST = []

              def memory_leak():
                  print("Starting memory leak...")
                  while True:
                      # Append 1MB of data every second
                      # This will eventually trigger OOMKill given limits
                      try:
                          LEAKY_LIST.append("x" * 1024 * 1024)
                      except MemoryError:
                          print("Memory error encountered (expected)")
                          pass
                      time.sleep(1)

              # Start leak in background
              threading.Thread(target=memory_leak, daemon=True).start()

              def inc_counter(name, labels):
                  key = tuple(sorted(labels.items()))
                  if key not in metrics[name]:
                      metrics[name][key] = 0.0
                  metrics[name][key] += 1.0

              def observe_histogram(name, labels, value):
                  # Simple sum/count for histogram
                  key = tuple(sorted(labels.items()))

                  sum_name = f"{name}_sum"
                  count_name = f"{name}_count"

                  if key not in metrics[sum_name]:
                      metrics[sum_name][key] = 0.0
                  if key not in metrics[count_name]:
                      metrics[count_name][key] = 0.0

                  metrics[sum_name][key] += value
                  metrics[count_name][key] += 1.0

              class DemoHandler(http.server.BaseHTTPRequestHandler):
                  def do_GET(self):
                      start_time = time.time()
                      status = 200

                      try:
                          if self.path == "/":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"status": "healthy", "app": APP_NAME}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/health":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"status": "ok", "timestamp": time.time()}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/ready":
                              self.send_response(200)
                              self.send_header("Content-type", "application/json")
                              self.end_headers()
                              response = {"ready": True}
                              self.wfile.write(json.dumps(response).encode())

                          elif self.path == "/metrics":
                              self.send_response(200)
                              self.send_header("Content-type", "text/plain")
                              self.end_headers()

                              # Generate Prometheus format
                              output = []

                              # Counter
                              output.append("# HELP http_requests_total Total HTTP requests")
                              output.append("# TYPE http_requests_total counter")
                              for key, value in metrics["http_requests_total"].items():
                                  label_str = ",".join([f'{k}="{v}"' for k, v in key])
                                  output.append(f'http_requests_total{{{label_str}}} {value}')

                              self.wfile.write("\n".join(output).encode())
                              return # Don't record metrics for metrics endpoint

                          else:
                              self.send_response(404)
                              self.end_headers()
                              status = 404

                          # Record metrics
                          # duration = time.time() - start_time
                          labels = {"method": "GET", "endpoint": self.path, "status": str(status)}
                          inc_counter("http_requests_total", labels)

                          # Histogram labels usually less cardinality
                          # hist_labels = {"endpoint": self.path}
                          # observe_histogram("http_request_duration_seconds", hist_labels, duration)

                      except Exception as e:
                          print(f"Error: {e}")
                          if not self.wfile.closed:
                              self.send_response(500)
                              self.end_headers()

              class ThreadedHTTPServer(socketserver.ThreadingMixIn, http.server.HTTPServer):
                  daemon_threads = True

              if __name__ == "__main__":
                  print(f"Starting {APP_NAME} on port {PORT}")
                  server = ThreadedHTTPServer(('0.0.0.0', PORT), DemoHandler)
                  server.serve_forever()
              EOF
              python /app/main.py
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          envFrom:
            - configMapRef:
                name: demo-api-config
            - secretRef:
                name: demo-api-secrets
          resources:
            requests:
              memory: "64Mi"
              cpu: "100m"
            limits:
              # PROBLEM: Limit is too low for the leak
              memory: "64Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
      restartPolicy: Always
