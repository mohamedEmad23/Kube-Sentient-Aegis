<<<<<<< HEAD
# AEGIS Environment Configuration
# Copy this file to .env and fill in your values
# See src/aegis/config/settings.py for all available options

# ===========================================================================
# Core Settings
# ===========================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# Enable debug logging
DEBUG=false

# ===========================================================================
# Kubernetes Configuration
# ===========================================================================

# Running inside cluster (auto-detected in most cases)
K8S_IN_CLUSTER=false

# Kubeconfig path (leave empty to use default ~/.kube/config)
K8S_KUBECONFIG_PATH=

# Kubernetes context (leave empty for current context)
K8S_CONTEXT=

# Namespace to watch (leave empty for all namespaces)
K8S_NAMESPACE=

# ===========================================================================
# Shadow Verification
# ===========================================================================

# Enable shadow verification workflows
SHADOW_ENABLED=true

# Shadow runtime (vcluster recommended)
SHADOW_RUNTIME=vcluster

# Namespace prefix for host shadow namespaces
SHADOW_NAMESPACE_PREFIX=aegis-shadow-

# Maximum concurrent shadow environments
SHADOW_MAX_CONCURRENT_SHADOWS=3

# Shadow verification timeout (seconds)
SHADOW_VERIFICATION_TIMEOUT=600

# Auto-cleanup shadow environments after verification
SHADOW_AUTO_CLEANUP=true

# ===========================================================================
# Incident Workflow
# ===========================================================================

# Auto-apply fixes after approval
INCIDENT_AUTO_FIX_ENABLED=false

# Approval timeout (minutes)
INCIDENT_APPROVAL_TIMEOUT_MINUTES=15

# Post-fix monitoring window (seconds)
INCIDENT_POST_FIX_MONITORING_SECONDS=300

# ===========================================================================
# Observability
# ===========================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
OBS_LOG_LEVEL=INFO

# Log format: json (production) or text (development)
OBS_LOG_FORMAT=text

# Enable Prometheus metrics endpoint
OBS_PROMETHEUS_ENABLED=true

# Metrics port
OBS_PROMETHEUS_PORT=8080

# Enable OpenTelemetry tracing
OBS_TRACING_ENABLED=false

# OTLP collector endpoint (if tracing enabled)
OBS_OTEL_EXPORTER_OTLP_ENDPOINT=

# Enable Loki log aggregation for RCA context
OBS_LOKI_ENABLED=false

# Loki base URL (e.g., http://loki:3100)
OBS_LOKI_URL=

# ===========================================================================
# Ollama Configuration
# ===========================================================================

# Ollama server endpoint
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use
OLLAMA_MODEL=phi3:mini

# Enable Ollama backend
OLLAMA_ENABLED=true

# Request timeout (seconds)
OLLAMA_TIMEOUT=300

# Max retry attempts
OLLAMA_MAX_RETRIES=3

# Sampling temperature (0.0-1.0)
OLLAMA_TEMPERATURE=0.2

# Context window size
OLLAMA_NUM_CTX=2048

# ===========================================================================
# Agent Models
# ===========================================================================

AGENT_RCA_MODEL=phi3:mini
AGENT_SOLUTION_MODEL=tinyllama:latest
AGENT_VERIFIER_MODEL=phi3:mini
AGENT_MAX_ITERATIONS=5
AGENT_TIMEOUT=300
AGENT_ENABLE_HUMAN_APPROVAL=false
AGENT_DRY_RUN_BY_DEFAULT=true

# ===========================================================================
# Load Testing
# ===========================================================================

LOADTEST_ENABLED=true
LOADTEST_USERS=10
LOADTEST_SPAWN_RATE=2
LOADTEST_DURATION=60
LOADTEST_TIMEOUT=10
LOADTEST_SUCCESS_THRESHOLD=0.95

# ===========================================================================
# Security Scanning
# ===========================================================================

# Enable Trivy image scanning during shadow verification
SECURITY_TRIVY_ENABLED=true

# Comma-separated severities that fail the gate
SECURITY_TRIVY_SEVERITY=HIGH,CRITICAL

# Enable Kubesec manifest scanning
SECURITY_KUBESEC_ENABLED=true

# Minimum Kubesec score to pass
SECURITY_KUBESEC_MIN_SCORE=0

# Enable Falco runtime checks
SECURITY_FALCO_ENABLED=true

# Falco severity threshold (WARNING, ERROR, CRITICAL, etc.)
SECURITY_FALCO_SEVERITY_THRESHOLD=WARNING

# Falco namespace and label selector
SECURITY_FALCO_NAMESPACE=falco
SECURITY_FALCO_LABEL_SELECTOR=app=falco

# ===========================================================================
# GPU Configuration
# ===========================================================================

GPU_ENABLED=false
GPU_COMPUTE_MODE=auto
GPU_DEVICE_TYPE=cuda
GPU_MEMORY_FRACTION=0.8
=======
# AEGIS Environment Configuration
# Copy this file to .env and fill in your values
#
# All AEGIS settings use the AEGIS_ prefix
# See src/aegis/config/settings.py for all available options

# ===========================================================================
# Core Settings
# ===========================================================================

# Environment: development, staging, production
AEGIS_ENVIRONMENT=development

# Shadow mode (dry-run) - HIGHLY RECOMMENDED to keep enabled initially
AEGIS_SHADOW_MODE_ENABLED=true

# Auto remediation - Only enable after thorough testing with shadow mode
AEGIS_AUTO_REMEDIATION_ENABLED=false

# ===========================================================================
# LLM Provider Configuration
# ===========================================================================

# Provider: ollama, groq, gemini, openai, together
AEGIS_LLM_PROVIDER=ollama

# LLM Temperature (0.0-2.0, lower = more deterministic)
AEGIS_LLM_TEMPERATURE=0.1

# Maximum tokens in response
AEGIS_LLM_MAX_TOKENS=4096

# ===========================================================================
# Ollama Configuration (for local GPU inference)
# ===========================================================================

# Ollama server endpoint
AEGIS_LLM_OLLAMA_HOST=http://localhost:11434

# Ollama model to use (run 'make gpu-check' for recommendations)
AEGIS_LLM_OLLAMA_MODEL=llama3.2:3b

# ===========================================================================
# Cloud LLM API Keys (for cloud inference)
# ===========================================================================

# Groq (RECOMMENDED - Free tier, fastest inference)
# Get your key at: https://console.groq.com/
AEGIS_LLM_GROQ_API_KEY=

# Google Gemini (RECOMMENDED - Free tier, large context)
# Get your key at: https://aistudio.google.com/apikey
AEGIS_LLM_GEMINI_API_KEY=

# OpenAI (Optional - Industry standard, paid)
# Get your key at: https://platform.openai.com/api-keys
AEGIS_LLM_OPENAI_API_KEY=

# Together AI (Optional - Open source models)
# Get your key at: https://api.together.xyz/
AEGIS_LLM_TOGETHER_API_KEY=

# ===========================================================================
# Kubernetes Configuration
# ===========================================================================

# Running inside cluster (auto-detected in most cases)
AEGIS_K8S_IN_CLUSTER=false

# Kubeconfig path (leave empty to use default ~/.kube/config)
AEGIS_K8S_KUBECONFIG=

# Kubernetes context (leave empty for current context)
AEGIS_K8S_CONTEXT=

# Namespace to watch (leave empty for all namespaces)
AEGIS_K8S_NAMESPACE=

# ===========================================================================
# Observability
# ===========================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
AEGIS_OBSERVABILITY_LOG_LEVEL=INFO

# Log format: json (production) or console (development)
AEGIS_OBSERVABILITY_LOG_FORMAT=console

# Enable Prometheus metrics endpoint
AEGIS_OBSERVABILITY_METRICS_ENABLED=true

# Metrics port
AEGIS_OBSERVABILITY_METRICS_PORT=8080

# Enable OpenTelemetry tracing
AEGIS_OBSERVABILITY_TRACING_ENABLED=false

# OTLP collector endpoint (if tracing enabled)
AEGIS_OBSERVABILITY_OTLP_ENDPOINT=http://localhost:4317

# ===========================================================================
# Shadow Mode Configuration
# ===========================================================================

# Enable shadow mode for all actions
AEGIS_SHADOW_ENABLED=true

# Require human verification before executing actions
AEGIS_SHADOW_VERIFICATION_REQUIRED=true

# Minimum confidence to auto-approve actions (0.0-1.0)
AEGIS_SHADOW_CONFIDENCE_THRESHOLD=0.85

# Maximum auto-approved actions per hour
AEGIS_SHADOW_MAX_AUTO_ACTIONS_PER_HOUR=10

# ===========================================================================
# Team Setup Examples
# ===========================================================================

# Example 1: Developer with NVIDIA GPU (6GB+ VRAM)
# AEGIS_LLM_PROVIDER=ollama
# AEGIS_LLM_OLLAMA_HOST=http://localhost:11434
# AEGIS_LLM_OLLAMA_MODEL=llama3.2:3b

# Example 2: Developer with Intel Iris Xe (cloud APIs only)
# AEGIS_LLM_PROVIDER=groq
# AEGIS_LLM_GROQ_API_KEY=your_groq_key

# Example 3: CI/CD pipeline
# AEGIS_LLM_PROVIDER=groq
# AEGIS_LLM_GROQ_API_KEY=your_groq_key
# AEGIS_SHADOW_ENABLED=true
>>>>>>> main
